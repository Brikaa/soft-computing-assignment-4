- We have training data
  - To train the neural network
- We have testing data
  - To give an overview of the accuracy after the training is done

- We have layers
- We have nodes in each layer
- We have an activation function in each node to limit the output (to avoid linearity)
  - What if my output is not bounded to 0 and 1 and I use sigmoid?
    - Don't use sigmoid in last layer

- For each epoch
  - For each row
    - Forward propagation
      - For each layer
        - For each node calculate node_in
          (weighted sum of node outs of previous layer or weighted sum of inputs if first layer)
        - Calculate node_out by applying activation function on node_in
      - Calculate sum of squared error (how much each output deviates from the expected one)
    - Backward propagation
      - For each weight, calculate how much it should be changed

- wds_new = wds + η * error_d * output_s
- error_d (output layer) = derivative_cost(output_d, expected_d) * derivative_act_of_d's_layer(output_d)
- error_d (hidden layer) = (sum(error_i * wid for i in neighbors of d)) * derivative_act_of_d's_layer(output_d)

- derivative_act is always the rate of change of activation function with respect to the input var

- Whole lifetime storage
  - Features
  - Need to store weights between neurons
  - Activation function for each layer
  - Cost function
- Each row lifetime storage
  - Need to store outputs for each neuron
  - Need to store errors for (dropped after row is done)
  - Inputs can be transient

- For each epoch
  - For each row
    - Outputs of first layer are the same as inputs
    - Forward propagation
      - For each layer after input layer
        - Prev layer size: n, current layer size: m
        - Output of previous layer represented as 1*n matrix, weights represented as n*m matrix
          - The ith column is the group of weights going into the ith node in the current layer
        - Multiply two matrices to get a 1*m matrix
          - The ith column (element) of the result matrix is the ith node input in the current layer
        - For each element (column) in the result matrix
          - ith node output = activation_function_of_that_layer(element)
    - Backward propagation
      - In output layer
        - For each node i
          - error node i = derivative_cost(output node i) * derivative_act_of_output_layer(output node i)
      - For each layer l before output layer inverted loop
        - For each node i
          - error node i = (sum(error_j * wji for j in neighbors of i)) * derivative_act_of_l(output node i)
      - For each weight wds
        - wds = wds + η * error_d * output_s


- ActivationFunction {
  apply: fn(float) -> float
  apply_derivative: fn(float) -> float
}

- CostFunction {
  apply: fn(float, float) -> float
  apply_derivative: fn(float, float) -> float
}

- Layer: {
  size: uint
  weights_in: arr[][size]f64
  inputs: arr[size]f64
  outputs: arr[size]f64
  errors: arr[size]f64
  activation_function: ActivationFunction
}

- Row: {
  inputs: []f64
  outputs: []f64
}

- Dataset = Row[]

- NeuralNetwork: {
  layers: Layer[]
  training_dataset: Dataset
  testing_dataset: Dataset
}

# weights[i][j] is the weight going from the previous layer of node i to the current layer of node j
# Measuring accuracy is applying summing cost function on all output layers on all rows

- get_input_layer_mut(layers)
  - return layers[0]

- get_output_layer_mut(layers)
  - return layers[layers.len() - 1]

- forward_propagation(features, layers)
  - get_input_layer_mut(layers).outputs = features.clone()
  - For l 1..layers.len()
    - layer = mutable reference layers[l]
    - layer.inputs = layers[l - 1].outputs * layer.weights_in
    - layer.outputs = inputs.map(inp => layer.activation_function.apply(inp))

- backward_propagation(learning_rate, cost_function, targets, layers)
  - output_layer = get_output_layer_mut(layers)
  - for i = 0..output_layer.outputs.size()
    - output = output_layer.outputs[i]
    - output_layer.errors[i] = cost_function.apply_derivative(output, targets[i])
      * output_layer.activation_function.apply_derivative(output_layer.inputs[i], output)
  - for l = layers.size()-2..=1
    - layer = mutable reference layers[l]
    - neighbor_layer = layers[l + 1]
    - for n = 0..layer.size
      - weighted_errors_sum = 0
      - for i = 0..layers[l + 1].size
        - weighted_errors_sum += neighbor_layer.weights_in[n][i] * neighbor_layer.errors[i]
      - layer.errors[n] = weighted_errors_sum * layer.activation_function.apply_derivative(layer.inputs[n], layer.outputs[n])
  - for l = 1..layers.size()
    - layer = layers[l]
    - weights = mutable reference layer.weights_in
    - for prev_node_index = 0..weights.len()
      - for current_node_index = 0..weights_in[prev_node_index].len()
        - weights[prev_node_index][current_node_index] +=
          learning_rate * layer.errors[current_node_index] * layers[l-1].outputs[prev_node_index]

- get_error(testing_dataset, cost_function, layers)
  - error = 0
  - For row in testing_dataset
    - forward_propagation(row.inputs, layers)
    - output_layer = get_output_layer_mut(layers)
    - for i 0..output_layer.size
      - error += cost_function.apply(output_layer.outputs[i], row.outputs[i])
    - return error


- NeuralNetwork.train(no_epochs, learning_rate, cost_function)
  - layers = mutable reference self.layers
  - For epoch 1..=no_epochs
    - For row in self.training_dataset
      - forward_propagation(row.inputs, layers)
      - backward_propagation(learning_rate, cost_function, row.outputs, layers)
    - error = get_error(self.testing_dataset, cost_function, layers)
    - print("Epoch #$epoch, error: $error")

- NeuralNetwork.new()
  - return self {
    layers: [],
    training_dataset: [],
    testing_dataset: []
  }

- NeuralNetwork.add_layer(size, activation_function)
  - if self.layers.len() == 0
    - weights_in = [0][0]
  - else
    - weights_in = vec[self.layers[self.layers.len() - 1].size][size]
  - outputs = vec[size]
  - errors = vec[size]
  - inputs = vec[size]
  - self.layers.push(Layer { size, weights_in, outputs, errors, activation_function, inputs })
  - return self

- NeuralNetwork.add_row(inputs, outputs, dataset)
  - if self.layers.len() < 2
    - panic("At least an input and output layer must exist before adding a dataset rot")
  - if inputs.len() != get_input_layer(self.layers).len()
    - panic("Size of dataset inputs mismatches with size of input layer")
  - if outputs.len() != get_output_Layer(self.layers).len()
    - panic("Size of dataset outputs mismatches with size of output layer")
  - dataset.push(Row { inputs, outputs })
  - return self

- NeuralNetwork.add_training_row(inputs, outputs)
  - return self.add_row(inputs, outputs, self.training_dataset)

- NeuralNetwork.add_training_row(inputs, outputs)
  - return self.add_row(inputs, outputs, self.testing_dataset)

- NeuralNetwork.get_outputs()
  - return get_output_layer(self.layers).outputs.clone()

- main
  - neural_network = NeuralNetwork::new()
  - neural_network
    .add_layer(4, sigmoid)
    .add_layer(8, sigmoid)
    .add_layer(1, linear)
  - for i in 1..=2
  - read no_training_inputs
  - while no_training_inputs--
    - i1, i2, i3, i4 = read
    - o = read
    - inputs = [i1, i2, i3, i4]
    - outputs = [o]
    - if i == 1
      - neural_network.add_training_row(inputs, outputs)
    - else
      - neural_network.add_testing_row(inputs, outputs)
  - neural_network.train(11, 0.1, mean_squared_error)
  - while true
    - i = read float
    - neural_network.forward_propagation(i)
    - print(neural_network.get_outputs())
