- We have training data
  - To train the neural network
- We have testing data
  - To give an overview of the accuracy after the training is done

- We have layers
- We have nodes in each layer
- We have an activation function in each node to limit the output (to avoid linearity)
  - What if my output is not bounded to 0 and 1 and I use sigmoid?
    - Don't use sigmoid in last layer

- For each epoch
  - For each row
    - Forward propagation
      - For each layer
        - For each node calculate node_in
          (weighted sum of node outs of previous layer or weighted sum of inputs if first layer)
        - Calculate node_out by applying activation function on node_in
      - Calculate sum of squared error (how much each output deviates from the expected one)
    - Backward propagation
      - For each weight, calculate how much it should be changed

- wds_new = wds + Î· * error_d * output_s
- error_d (output layer) = (expected_d - output_d) * derivative_act_of_d's_layer(output_d)
- error_d (hidden layer) = (sum(error_i * wid for i in neighbors of d)) * derivative_act_of_d's_layer(output_d)

- derivative_act is always the rate of change of activation function with respect to the input var
