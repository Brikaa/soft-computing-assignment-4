- We have training data
  - To train the neural network
- We have testing data
  - To give an overview of the accuracy after the training is done

- We have layers
- We have nodes in each layer
- We have an activation function in each node to limit the output (to avoid linearity)
  - What if my output is not bounded to 0 and 1 and I use sigmoid?
    - Don't use sigmoid in last layer

- For each epoch
  - For each row
    - Forward propagation
      - For each layer
        - For each node calculate node_in
          (weighted sum of node outs of previous layer or weighted sum of inputs if first layer)
        - Calculate node_out by applying activation function on node_in
      - Calculate sum of squared error (how much each output deviates from the expected one)
    - Backward propagation
      - For each weight, calculate how much it should be changed

- wab_k_new = wab_k + (error_b_(k+1) * output_a_k)
- error_b_k (output layer sigmoid) = output_b_k * (1 - output_b_k) * (target_b - output_b_k)
- error_b_k (output layer linear) = target_b - output_b_k
- error_b_k (hidden layer sigmoid) = output_b_k * (1 - output_b_k) * (error_d_(k+1) * wbd_k for d in neighbors of b)
