- We have training data
  - To train the neural network
- We have testing data
  - To give an overview of the accuracy after the training is done

- We have layers
- We have nodes in each layer
- We have an activation function in each node to limit the output (to avoid linearity)
  - What if my output is not bounded to 0 and 1 and I use sigmoid?
    - Don't use sigmoid in last layer

- For each epoch
  - For each row
    - Forward propagation
      - For each layer
        - For each node calculate node_in
          (weighted sum of node outs of previous layer or weighted sum of inputs if first layer)
        - Calculate node_out by applying activation function on node_in
      - Calculate sum of squared error (how much each output deviates from the expected one)
    - Backward propagation
      - For each weight, calculate how much it should be changed

- wds_new = wds + η * error_d * output_s
- error_d (output layer) = derivative_cost(output_d, expected_d) * derivative_act_of_d's_layer(output_d)
- error_d (hidden layer) = (sum(error_i * wid for i in neighbors of d)) * derivative_act_of_d's_layer(output_d)

- derivative_act is always the rate of change of activation function with respect to the input var

- Whole lifetime storage
  - Features
  - Need to store weights between neurons
  - Activation function for each layer
  - Cost function
- Each row lifetime storage
  - Need to store outputs for each neuron
  - Need to store errors for (dropped after row is done)
  - Inputs can be transient

- For each epoch
  - For each row
    - Outputs of first layer are the same as inputs
    - Forward propagation
      - For each layer after input layer
        - Prev layer size: n, current layer size: m
        - Output of previous layer represented as 1*n matrix, weights represented as n*m matrix
          - The ith column is the group of weights going into the ith node in the current layer
        - Multiply two matrices to get a 1*m matrix
          - The ith column (element) of the result matrix is the ith node input in the current layer
        - For each element (column) in the result matrix
          - ith node output = activation_function_of_that_layer(element)
    - Backward propagation
      - In output layer
        - For each node i
          - error node i = derivative_cost(output node i) * derivative_act_of_output_layer(output node i)
      - For each layer l before output layer inverted loop
        - For each node i
          - error node i = (sum(error_j * wji for j in neighbors of i)) * derivative_act_of_l(output node i)
      - For each weight wds
        - wds = wds + η * error_d * output_s


- ActivationFunction {
  apply: fn(float) -> float
  apply_derivative: fn(float) -> float
}

- CostFunction {
  apply: fn(float, float) -> float
  apply_derivative: fn(float, float) -> float
}

- Layer: {
  size: uint
  weights_in: arr[][size]f64
  outputs: arr[size]f64
  errors: arr[size]f64
  activation_function: ActivationFunction
}

- Row: {
  inputs: []f64
  outputs: []f64
}

- Dataset = Row[]
- Neural
- Neural network: {
  layers: Layer[]
  cost_function: DerivableFunction
  learning_rate: float
}

# weights[i][j] is the weight going from the previous layer of node i to the current layer of node j

- train(no_epochs, learning_rate, neural_network, dataset)
  - layers = neural_network.layers
  - For epoch 1..=no_epochs
    - For row in dataset
      - layers[0].outputs = row.inputs
      # Forward propagation
      - For l 1..layers.len()
        - layer = mutable reference layers[l]
        - inputs = layers[l - 1].outputs * layer.weights_in
        - layer.outputs = inputs.map(inp => layer.activation_function.apply(inp))
      # Backward propagation
      - output_layer = mutable reference layers[layers.size() - 1]
      - for i = 0..output_layer.outputs.size()
        - output = output_layer.outputs[i]
        - output_layer.errors[i] = neural_network.cost_function.apply_derivative(output, row.outputs[i])
          * output_layer.activation_function.apply_derivative(output)
      - for l = layers.size()-2..=0
        - layer = mutable reference layers[l]
        - neighbor_layer = layers[l + 1]
        - for n = 0..layer.size
          - weighted_errors_sum = 0
          - for i = 0..layers[l + 1].size
            - weighted_errors_sum = neighbor_layer.weights[n][i] * neighbor_layer.errors[i]
          - layer.errors[n] = weighted_sum * layer.activation_function.apply_derivative(layer.outputs[n])
      - for l = 1..layers.size()
        - layer = layers[l]
        - weights = mutable reference layer.weights
        - for prev_node_index = 0..weights.len()
          - for current_node_index = 0..weights[prev_node_index].len()
            - weights[prev_node_index][current_node_index] +=
              learning_rate * layer.errors[current_node_index] * layers[l-1].outputs[prev_node_index]
